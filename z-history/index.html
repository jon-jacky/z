<html>
<head>
<link rel="stylesheet" href="zed.css" type="text/css">
<title>Z History</title>
</head>

<body>

<h3>Z History</h3>

<a href="https://jon-jacky.github.io/home/">Jon Jacky</a>

<p>
Annotated links to sources (by others, not me) that document
significant moments in the history of the Z notation.  They trace the
growth, then decline, of interest in Z.
</p>

<p>
<a href="https://jon-jacky.github.io/z/">More pages</a> about Z.
</p>

<hr>

<a href="proof-in-z-martin-1997.pdf">Approaches to proof in Z</a> by
Andrew Martin (1997): "Various attempts at supporting proof in Z are
described in the literature. This paper presents a survey of these
approaches, and the underlying semantic issues which make proof in Z a
non-trivial task. ...  Special care is given to an account of the
peculiarities of Z schemas. ...  The broad conclusion is that none of
these approaches is a clear winner at present ..."
</p>

<p>
This thorough report surveys the work of the Z tool-making community
at the time of its greatest expansion.  It explains some difficult
technical issues in Z better than any other source I know.
</p>

<p>
I think this report also shows why interest in Z started to decline at
about this time.  Martin finds a proliferation of incompatible proof
tools of only prototype quality, whose convenience and power were
limited by technical and foundational issues.  This seemed to reveal
problems with Z itself, which might have discouraged its adoption.
How did this situation arise?  Here is how I remember it:
</p>

<p>
Z was supposed to be easy.  One of its creators said, "We just wanted
to use simple mathematics to describe software". Z began as nothing
more than some conventions for writing formulas about logic and sets,
which seemed to be straightforward and uncontroversial.
</p>

<p>
It is the mathematics that makes Z <em>formal</em>.  It promises to
make the power of mathematics available for analyzing specifications,
in a way that is not possible with prose.  Using mathematics for
analyzing and writing computer programs is called <em>formal methods</em>.
</p>

<p>
For some, Z was not just a notation, it was the vanguard innovation in
a movement to reform computer programming into a more rigorous,
science-based discipline.  They hoped that formal specifications might
play a similar role in programming that mathematical models play in
science and engineering.
</p>

<p>
The defining innovation in Z is a construct called
the <em>schema</em>, which attaches a name to a collection of
formulas.  An author can use this name in subsequent formulas
as if it were an ordinary identifier.  There is a <em>schema
calculus</em> where all of the usual logical operators can be
applied to schema names, with the meanings that intuition would
expect.  For example, to express that a radiation therapy machine
control system meets its safety requirements, one can write
<em>TherapyControl => SafeTreatment</em> (pronounced "therapy control
implies safe treatment"), where <em>TherapyControl</em> is the schema
name applied to many pages of formulas that describe the control
system as a collection of state transitions,
and <em>SafeTreatment</em> is the schema name applied to many more
pages that express the safety requirements as a collection of safe
states.  This formula says that the operations of the control system
can only reach safe states -- never an unsafe state.  Here the schema
names are used as predicates, but schema names can play <em>any</em>
role in a formula -- as declarations, expressions, or predicates -- so
their interpretation depends on context.
</p>

<p>
Authors found that they could use Z schemas to write formal
specifications that were not only precise, but also expressive and
pursuasive.  Readers found that schemas provided a structure that
could make formal specifications easier to grasp.  Z promised to
relieve the tedium and pedantry of writing and reading formal
specifications.  Advocates hoped it might help formal specifications
become an accepted part of the software development process.
</p>

<p>
Some authors were content to regard a Z specification as purely
descriptive: precise and well-structured documentation,
which could be more helpful for coding the program
than a prose description.  In this view, the
formula <em>TherapyControl => SafeTreatment</em> merely expresses the
designer's intent.  
</p>

<p>
Others were more ambitious.  They hoped to
to <em>prove</em> properties of the Z specification.  They would use
the methods of logical inference to check whether all the formulas
included in <em>TherapyControl</em> really do guarantee all the safety
requirements in <em>SafeTreatment</em>.  A successful proof should
provide strong confidence that the design expressed in the Z
specification really does specify a safe therapy machine -- that
nothing has been overlooked or written incorrectly.  In this view, the
main reason for writing a formal specification is to make it possible
to perform proofs like this.
</p>

<p>
Such proofs are usually lengthy, tedious, and error-prone.  They
consist of long sequences where rules of inference are applied to
formulas from (for example)
<em>TherapyControl</em> to derive new formulas, until eventually the
sequence reaches the formulas in <em>SafeTreatment</em>.  The proofs are so
tedious that most people would not even consider attempting them.
</p>

<p>
But there is a technology of <em>theorem prover</em> programs that
assist authors writing proofs, which might make the proofs feasible.
These programs check every proof step to confirm that the inference
rules have been correctly applied.  Some provers can even generate
some of the tedious proof steps, under the author's direction.  A
proof that has been checked by one of these programs is considered to
be valid, even if it is complicated and difficult to follow.
</p>

<p>
It seemed to some that the eventual success of the entire Z movement
depended on getting a useful theorem prover program for Z.  When Z
appeared, there were already a number of these theorem prover programs
in use, for ordinary logic without schemas.  Since Z was "just sets
and logic" and a schema just named a collection of ordinary formulas,
it seemed that adapting these provers to work with Z specifications
should be straightforward.  But this turned out to be more difficult
than expected.
</p>

<p>
To be acceptable to Z authors, the prover has to work with schemas,
not the underlying formulas.  But it was found that, due to various
technical issues, especially their context dependence, schemas and
their constituents cannot be handled like ordinary logical formulas.
Therefore, to adapt an existing prover, it is necessary to translate Z
into its native logic, which is difficult to do faithfully.
Alternatively, an entirely new prover tailored to Z can be developed
from scratch, requiring substantial effort.  Both approaches were
tried, in many variations.  This is the landscape surveyed in Martin's
review.
</p>

<p>
There was a felt need for a standard that would clarify the complications in Z
and provide a canonical interpretation for each.  
The standard should include a <em>semantics</em> that describes exactly what 
any Z text denoted, as a collection of variable names and their
values, after schemas had been eliminated.  It should also include 
a <em>deductive system</em> -- a "logic for Z" -- a collection of inference
rules for writing proofs that included schemas.
</p>

<p> 
A Z Committee was formed in 1992 to write a Z standard and get it
approved by an international standards organization, the ISO/IEC.  A
single author had already written <em>The Z Notation: A Reference
Manual</em> in 1989, with a second edition in 1992.  Usually called
the <em>ZRM</em>, it was the <em>de facto</em> standard, but it did
not provide a deductive system nor a semantics (although the same
author did propose a semantics in another book).  Moreover, some Z
authors were eager to use schemas even more liberally, in ways not
endorsed by the <em>ZRM</em>.  The Z Committee (which did not include
the author of the <em>ZRM</em>) chose not to simply codify the
existing <em>ZRM</em> in the format required by ISO, but to go
further -- to provide a deductive system and semantics, and
accommodate the new styles for using schemas.
</p>

<p>
The Z Committee had, in effect, undertaken a research project.  They
tried to innovate and standardize at the same time.  They worked on
through the 1990s, producing a series of inconclusive draft standards.
The deductive system proposed in one draft was found to be
inconsistent -- that is, erroneous -- and the committee eventually
decided not to include one.  They advised that any deductive system
would conform to the standard if it was consistent with the
semantics they defined -- but they declined to provide an example.
After several years, one frustrated tool builder wrote, "The Z
committee has missed the boat."  The standard was finally adopted in
2002 -- too late, some felt.
</p>

<p>
Meanwhile, lacking guidance from a standard, each group -- there were
many -- tried to cope with the difficulties in its own way.  No
consensus solution emerged. The community was fragmented.  The proof
tools were all incompatible.  Each tool used proof scripts that were
not usable by any other.  Even the kinds of properties that could be
proved varied from one tool to another.   This is the situation described
in Martin's report.
</p>

<p>
Several authors (outside the Z Committee) eventually did propose
different deductive systems that appeared to be sound, but by then
interest in Z was waning.  The revelations that Z was not so simple,
the long-delayed standard, and the proliferation of incompatible tools
may have contributed to its decline.  If you are designing a system,
you don't want your notation and tools to be somebody's idea of a
research problem.
</p>

<p>
The most ambitious aspirations of the Z movement were probably doomed
to disappointment.  The movement viewed formal methods as a Field of
Dreams: "Build it and they'll come."  But most software developers
were never going to write or use formal specifications in any
notation, because they know that most successful projects don't use
them.  Since the 1990s, the emphasis in the mainstream of software
development has moved even more toward getting quickly to code, then
trying to solve all the problems there.  So the travails I relate here
may not have been decisive in Z's decline.
</p>  

<p>
Getting developers to use a prover was even more unlikely.  It is not
much like programming; it is an esoteric and difficult skill, mastered
by few.  Since the 1980s, an alternative kind of analysis tool called
a <em>model checker</em> has emerged, which (in effect) performs
exhaustive testing (of specifications, not programs) over limited
domains.  Using a model checker is much easier for developers -- it is
more like running a program than proving a theorem.  Model checkers
attracted little interest in the Z community, but turned out to be
more effective than provers for detecting most errors.
</p>

<p>
Z is almost forgotten, but there are niches where formal
specifications are still used -- where some combination of novelty,
difficulty, and the cost of errors convinces developers that their
usual intuitive approach is not adequate.  But instead of Z, two other
notations that appeared in the 1990s were the most reported on in the
2010s.  Both
<a href="https://alloytools.org/citations/case-studies.html">Alloy</a>
and
<a href="https://lamport.azurewebsites.net/tla/industrial-use.html">TLA+</a>
are influenced by Z.  But unlike Z, both are the creations of a single
author who remains the leader of their community, provides focus, and
avoids fragmentation.  Both notations are simpler than Z and were developed
along with their tool sets, which both feature a model checker.  TLA+ also
has a prover, but its model checker is used more often.
</p>

<hr>

<p>
Revised Nov 2022
</p>
